{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import random  \n",
    "import ast  \n",
    "import itertools  \n",
    "import pickle  \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108719\n",
      "2000\n",
      "1406 237\n",
      "52463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n# to_word \\nword2idx, content_length, question_length, _ = pickle.load(open(\\'vocab.data\\', \"rb\")) \\n  \\ndef get_value(dic,value): \\n    for name in dic: \\n        if dic[name] == value: \\n            return name \\n  \\nwith open(\\'train.vec\\') as f: \\n    for line in f: \\n        line = ast.literal_eval(line.strip()) \\n        for word in line[0]: \\n            print(get_value(word2idx, word)) \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_file = './CBTest/data/cbtest_NE_train.txt'  \n",
    "valid_data_file = './CBTest/data/cbtest_NE_valid_2000ex.txt'  \n",
    "   \n",
    "def preprocess_data(data_file, out_file):  \n",
    "    # stories[x][0]  tories[x][1]  tories[x][2]  \n",
    "    stories = []  \n",
    "    with open(data_file) as f:  \n",
    "        story = []  \n",
    "        for line in f:  \n",
    "            line = line.strip()  \n",
    "            if not line:  \n",
    "                story = []  \n",
    "            else:  \n",
    "                _, line = line.split(' ', 1)  #按空格分，仅分割一次得到 序号 line\n",
    "                if line:  \n",
    "                    if '\\t' in line:  #有问题题的是以xxxxx是问题 \\t 答案 \\t\\t 选项\n",
    "                        q, a, _, answers = line.split('\\t')  \n",
    "                        # tokenize  \n",
    "                        q = [s.strip() for s in re.split('(\\W+)+', q) if s.strip()]  #()加括号会保留匹配项，(\\W+)是匹配数字字母和下划线汉了\n",
    "                        stories.append((story, q, a))  \n",
    "                    else:  \n",
    "                        line = [s.strip() for s in re.split('(\\W+)+', line) if s.strip()]  \n",
    "                        story.append(line)  \n",
    "   \n",
    "    samples = []  \n",
    "    for story in stories:  \n",
    "        story_tmp = []  \n",
    "        content = []  \n",
    "        for c in story[0]:  \n",
    "            content += c  \n",
    "        story_tmp.append(content)  #三个元素story q a\n",
    "        story_tmp.append(story[1])  \n",
    "        story_tmp.append(story[2])  \n",
    "   \n",
    "        samples.append(story_tmp)  \n",
    "   \n",
    "    random.shuffle(samples)  \n",
    "    print(len(samples))  #输出样例数\n",
    "   \n",
    "    with open(out_file, \"w\") as f:  \n",
    "        for sample in samples:  \n",
    "            f.write(str(sample))  #['content', q ,a] 一行一个样例sample\n",
    "            f.write('\\n')  \n",
    "   \n",
    "preprocess_data(train_data_file, 'train.data')  \n",
    "preprocess_data(valid_data_file, 'valid.data')  \n",
    "   \n",
    "# 创建词汇表  \n",
    "def read_data(data_file):  \n",
    "    stories = []  \n",
    "    with open(data_file) as f:  \n",
    "        for line in f:  \n",
    "            line = ast.literal_eval(line.strip())  #ast.literal_eval会合法转换\n",
    "            stories.append(line)  \n",
    "    return stories  \n",
    "   \n",
    "   \n",
    "stories = read_data('train.data') + read_data('valid.data')  \n",
    "   \n",
    "content_length = max([len(s) for s, _, _ in stories])  \n",
    "question_length = max([len(q) for _, q, _ in stories])  \n",
    "print(content_length, question_length)  \n",
    "\n",
    "#itertools是连接起来\n",
    "vocab = sorted(set(itertools.chain(*(story + q + [answer] for story, q, answer in stories))))  #一个*星号表示list或turple **map dict\n",
    "vocab_size = len(vocab) + 1  \n",
    "print(vocab_size)  #词表大小\n",
    "word2idx = dict((w, i + 1) for i,w in enumerate(vocab))  #字典(word : dx)\n",
    "pickle.dump((word2idx, content_length, question_length, vocab_size), open('vocab.data', \"wb\"))  #保存\n",
    "   \n",
    "# From keras 补齐  \n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',  \n",
    "                  padding='post', truncating='post', value=0.):  \n",
    "    lengths = [len(s) for s in sequences]  \n",
    "   \n",
    "    nb_samples = len(sequences)  #样例数\n",
    "    if maxlen is None:  \n",
    "        maxlen = np.max(lengths)  #最长的样例长度\n",
    "   \n",
    "    # take the sample shape from the first non empty sequence  \n",
    "    # checking for consistency in the main loop below.  \n",
    "    sample_shape = tuple()  \n",
    "    for s in sequences:  \n",
    "        if len(s) > 0:  \n",
    "            sample_shape = np.asarray(s).shape[1:]  \n",
    "            break  \n",
    "   \n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)  \n",
    "    for idx, s in enumerate(sequences):  \n",
    "        if len(s) == 0:  \n",
    "            continue  # empty list was found  \n",
    "        if truncating == 'pre':  \n",
    "            trunc = s[-maxlen:]  \n",
    "        elif truncating == 'post':  \n",
    "            trunc = s[:maxlen]  #s没有maxlen,会取到句子的最后一个元素\n",
    "        else:  \n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)  \n",
    "   \n",
    "        # check `trunc` has expected shape  \n",
    "        trunc = np.asarray(trunc, dtype=dtype)  \n",
    "        if trunc.shape[1:] != sample_shape:  \n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %  \n",
    "                             (trunc.shape[1:], idx, sample_shape))  \n",
    "   \n",
    "        if padding == 'post':  \n",
    "            x[idx, :len(trunc)] = trunc  #就是每story的长度是一样的，主要是xshape是(样例数， 最大长度)的一个矩阵，让trunc赋值给这个矩阵\n",
    "        elif padding == 'pre':  \n",
    "            x[idx, -len(trunc):] = trunc  \n",
    "        else:  \n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)  \n",
    "    return x  \n",
    "   \n",
    "# 转为向量  \n",
    "def to_vector(data_file, output_file):  \n",
    "    word2idx, content_length, question_length, _ = pickle.load(open('vocab.data', \"rb\"))  \n",
    "      \n",
    "    X = []  \n",
    "    Q = []  \n",
    "    A = []  \n",
    "    with open(data_file) as f_i:  \n",
    "        for line in f_i:  \n",
    "            line = ast.literal_eval(line.strip())  \n",
    "            x = [word2idx[w] for w in line[0]]  #转为向量，词转为向量\n",
    "            q = [word2idx[w] for w in line[1]]  \n",
    "            a = [word2idx[line[2]]]  \n",
    "   \n",
    "            X.append(x)  \n",
    "            Q.append(q)  \n",
    "            A.append(a)  \n",
    "   \n",
    "    X = pad_sequences(X, content_length)  \n",
    "    Q = pad_sequences(Q, question_length)  \n",
    "   \n",
    "    with open(output_file, \"w\") as f_o:  \n",
    "        for i in range(len(X)):  \n",
    "            f_o.write(str([X[i].tolist(), Q[i].tolist(), A[i]]))  #每行是X Q A\n",
    "            f_o.write('\\n')  \n",
    "   \n",
    "to_vector('train.data', 'train.vec')  \n",
    "to_vector('valid.data', 'valid.vec')  \n",
    "   \n",
    "   \n",
    "\"\"\" \n",
    "# to_word \n",
    "word2idx, content_length, question_length, _ = pickle.load(open('vocab.data', \"rb\")) \n",
    "  \n",
    "def get_value(dic,value): \n",
    "    for name in dic: \n",
    "        if dic[name] == value: \n",
    "            return name \n",
    "  \n",
    "with open('train.vec') as f: \n",
    "    for line in f: \n",
    "        line = ast.literal_eval(line.strip()) \n",
    "        for word in line[0]: \n",
    "            print(get_value(word2idx, word)) \n",
    "\"\"\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [1,2,2]\n",
    "a = [1, 2, 3]\n",
    "b = np.asarray(a)\n",
    "b.shape[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "import pickle  \n",
    "import numpy as np  \n",
    "import ast  \n",
    "from collections import defaultdict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406 237 52463\n",
      "没找到模型\n",
      "6.43264\n",
      "./machine_reading.model-0\n",
      "0.0\n",
      "6.46347\n",
      "5.8864\n",
      "5.75152\n",
      "5.62566\n",
      "5.44342\n",
      "5.37489\n",
      "5.13651\n",
      "5.04464\n",
      "4.93321\n"
     ]
    }
   ],
   "source": [
    "train_data = 'train.vec'  \n",
    "valid_data = 'valid.vec'  \n",
    "   \n",
    "word2idx, content_length, question_length, vocab_size = pickle.load(open('vocab.data', \"rb\"))  \n",
    "print(content_length, question_length, vocab_size)  \n",
    "   \n",
    "batch_size = 64  #每个batch_size是64\n",
    "   \n",
    "train_file = open(train_data)  \n",
    "def get_next_batch():  \n",
    "    X = []  \n",
    "    Q = []  \n",
    "    A = []  \n",
    "    for i in range(batch_size):  \n",
    "        for line in train_file:  \n",
    "            line = ast.literal_eval(line.strip())  \n",
    "            X.append(line[0])  \n",
    "            Q.append(line[1])  \n",
    "            A.append(line[2][0])  #没有用到选项\n",
    "            break  \n",
    "   \n",
    "    if len(X) == batch_size:  \n",
    "        return X, Q, A  \n",
    "    else:  \n",
    "        train_file.seek(0)  \n",
    "        return get_next_batch()  \n",
    "   \n",
    "def get_test_batch():  \n",
    "    with open(valid_data) as f:  \n",
    "        X = []  \n",
    "        Q = []  \n",
    "        A = []  \n",
    "        for line in f:  \n",
    "            line = ast.literal_eval(line.strip())  \n",
    "            X.append(line[0])  \n",
    "            Q.append(line[1])  \n",
    "            A.append(line[2][0])  \n",
    "        return X, Q, A  #返回X Q A都是词向量\n",
    "   \n",
    "   \n",
    "X = tf.placeholder(tf.int32, [batch_size, content_length])   # 材料  \n",
    "Q = tf.placeholder(tf.int32, [batch_size, question_length])  # 问题  \n",
    "A = tf.placeholder(tf.int32, [batch_size])                   # 答案  \n",
    "   \n",
    "# drop out  \n",
    "keep_prob = tf.placeholder(tf.float32)  \n",
    "   \n",
    "def glimpse(weights, bias, encodings, inputs):  \n",
    "    weights = tf.nn.dropout(weights, keep_prob)  \n",
    "    inputs = tf.nn.dropout(inputs, keep_prob)  \n",
    "    attention = tf.transpose(tf.matmul(weights, tf.transpose(inputs)) + bias)  \n",
    "    attention = tf.matmul(encodings, tf.expand_dims(attention, -1))  \n",
    "    attention = tf.nn.softmax(tf.squeeze(attention, -1))  \n",
    "    return attention, tf.reduce_sum(tf.expand_dims(attention, -1) * encodings, 1)  \n",
    "   \n",
    "def neural_attention(embedding_dim=384, encoding_dim=128):  #对每个词embeding吧\n",
    "    embeddings = tf.Variable(tf.random_normal([vocab_size, embedding_dim], stddev=0.22), dtype=tf.float32)  \n",
    "    tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-4), [embeddings])  #应用正则化到参数上\n",
    "   \n",
    "    with tf.variable_scope('encode'):  \n",
    "        with tf.variable_scope('X'):  \n",
    "            X_lens = tf.reduce_sum(tf.sign(tf.abs(X)), 1)  #X的长度\n",
    "            embedded_X = tf.nn.embedding_lookup(embeddings, X)  #讲就是根据input_ids中的id，寻找embedding中的对应元素\n",
    "            encoded_X = tf.nn.dropout(embedded_X, keep_prob)  \n",
    "            gru_cell = tf.nn.rnn_cell.GRUCell(encoding_dim)  #GRUCELL\n",
    "            outputs, output_states = tf.nn.bidirectional_dynamic_rnn(gru_cell, gru_cell, encoded_X, sequence_length=X_lens, dtype=tf.float32, swap_memory=True)  \n",
    "            encoded_X = tf.concat(outputs,2)  #encoded\n",
    "        with tf.variable_scope('Q'):  \n",
    "            Q_lens = tf.reduce_sum(tf.sign(tf.abs(Q)), 1)  \n",
    "            embedded_Q = tf.nn.embedding_lookup(embeddings, Q)  \n",
    "            encoded_Q = tf.nn.dropout(embedded_Q, keep_prob)  \n",
    "            gru_cell = tf.nn.rnn_cell.GRUCell(encoding_dim)  \n",
    "            outputs, output_states = tf.nn.bidirectional_dynamic_rnn(gru_cell, gru_cell, encoded_Q, sequence_length=Q_lens, dtype=tf.float32, swap_memory=True)  \n",
    "            encoded_Q = tf.concat( outputs,2)  \n",
    "   \n",
    "    W_q = tf.Variable(tf.random_normal([2*encoding_dim, 4*encoding_dim], stddev=0.22), dtype=tf.float32)  \n",
    "    b_q = tf.Variable(tf.random_normal([2*encoding_dim, 1], stddev=0.22), dtype=tf.float32)  \n",
    "    W_d = tf.Variable(tf.random_normal([2*encoding_dim, 6*encoding_dim], stddev=0.22), dtype=tf.float32)  \n",
    "    b_d = tf.Variable(tf.random_normal([2*encoding_dim, 1], stddev=0.22), dtype=tf.float32)  \n",
    "    g_q = tf.Variable(tf.random_normal([10*encoding_dim, 2*encoding_dim], stddev=0.22), dtype=tf.float32)  \n",
    "    g_d = tf.Variable(tf.random_normal([10*encoding_dim, 2*encoding_dim], stddev=0.22), dtype=tf.float32)  \n",
    "   \n",
    "    with tf.variable_scope('attend') as scope:  \n",
    "        infer_gru = tf.nn.rnn_cell.GRUCell(4*encoding_dim)  \n",
    "        infer_state = infer_gru.zero_state(batch_size, tf.float32)  \n",
    "        for iter_step in range(8):  \n",
    "            if iter_step > 0:  \n",
    "                scope.reuse_variables()  \n",
    "   \n",
    "            _, q_glimpse = glimpse(W_q, b_q, encoded_Q, infer_state)  \n",
    "            d_attention, d_glimpse = glimpse(W_d, b_d, encoded_X, tf.concat([infer_state, q_glimpse], 1))  \n",
    "   \n",
    "            gate_concat = tf.concat([infer_state, q_glimpse, d_glimpse, q_glimpse * d_glimpse], 1)  \n",
    "   \n",
    "            r_d = tf.sigmoid(tf.matmul(gate_concat, g_d))  \n",
    "            r_d = tf.nn.dropout(r_d, keep_prob)  \n",
    "            r_q = tf.sigmoid(tf.matmul(gate_concat, g_q))  \n",
    "            r_q = tf.nn.dropout(r_q, keep_prob)  \n",
    "   \n",
    "            combined_gated_glimpse = tf.concat([r_q * q_glimpse, r_d * d_glimpse], 1)  \n",
    "            _, infer_state = infer_gru(combined_gated_glimpse, infer_state)  \n",
    "   \n",
    "    return tf.to_float(tf.sign(tf.abs(X))) * d_attention  \n",
    "   \n",
    "def train_neural_attention():  \n",
    "    X_attentions = neural_attention()  \n",
    "    loss = -tf.reduce_mean(tf.log(tf.reduce_sum(tf.to_float(tf.equal(tf.expand_dims(A, -1), X)) * X_attentions, 1) + tf.constant(0.00001)))  \n",
    "   \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)  \n",
    "    grads_and_vars = optimizer.compute_gradients(loss)  \n",
    "    capped_grads_and_vars = [(tf.clip_by_norm(g, 5), v) for g,v in grads_and_vars]  \n",
    "    train_op = optimizer.apply_gradients(capped_grads_and_vars)  \n",
    "   \n",
    "    saver = tf.train.Saver()  \n",
    "    with tf.Session() as sess:  \n",
    "        sess.run(tf.global_variables_initializer())  \n",
    "   \n",
    "        # writer = tf.summary.FileWriter()  \n",
    "        # 恢复前一次训练  \n",
    "        ckpt = tf.train.get_checkpoint_state('.')  \n",
    "        if ckpt != None:  \n",
    "            print(ckpt.model_checkpoint_path)  \n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)  \n",
    "        else:  \n",
    "            print(\"没找到模型\")  \n",
    "   \n",
    "        for step in range(20000):  \n",
    "            train_x, train_q, train_a = get_next_batch()  \n",
    "            loss_, _ = sess.run([loss, train_op], feed_dict={X:train_x, Q:train_q, A:train_a, keep_prob:0.7})  \n",
    "            print(loss_)  \n",
    "   \n",
    "            # 保存模型并计算准确率  \n",
    "            if step % 1000 == 0:  \n",
    "                path = saver.save(sess, './machine_reading.model', global_step=step)  \n",
    "                print(path)  \n",
    "   \n",
    "                test_x, test_q, test_a = get_test_batch()  \n",
    "                test_x, test_q, test_a = np.array(test_x[:batch_size]), np.array(test_q[:batch_size]), np.array(test_a[:batch_size])  \n",
    "                attentions = sess.run(X_attentions, feed_dict={X:test_x, Q:test_q, keep_prob:1.})  \n",
    "                correct_count = 0  \n",
    "                for x in range(test_x.shape[0]):  \n",
    "                    probs = defaultdict(int)  \n",
    "                    for idx, word in enumerate(test_x[x,:]):  \n",
    "                        probs[word] += attentions[x, idx]  \n",
    "                    guess = max(probs, key=probs.get)  \n",
    "                    if guess == test_a[x]:  \n",
    "                        correct_count += 1  \n",
    "                print(correct_count / test_x.shape[0])  \n",
    "   \n",
    "train_neural_attention()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
